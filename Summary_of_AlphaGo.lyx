#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Research Review: AlphaGo by the DeepMind Team
\end_layout

\begin_layout Paragraph*
Summary of AlphaGo
\end_layout

\begin_layout Standard
AlphaGo is based on a combination of deep neural networks and Monte-Carlo
 tree search.
 Game of Go has long been viewed as the most challenging of classic games
 for artificial intelligence for two main reasons: 1.
 Search space for playing Go is really huge - bd (b≈250, d≈150) where b
 is the game’s breadth and d is its depth (game length).
 2.
 Complexity to write evaluation function to determine who is winning.
 
\end_layout

\begin_layout Standard
DeepMind team overcomes these two problem using two trained neural networks:
\end_layout

\begin_layout Standard
1.
 Policy Network: Policy network provides the probability distribution of
 moves.
 It learns to predict - for any particular position what’s the most likely
 moves were to be played.
 So instead of looking at one position of all possible legal moves, it looks
 for top 3 or top 5 most likely moves were taken into consideration.This
 reduces down the breadth of the search space.
 Policy network is first trained by Supervised learning using 30 million
 positions data from the KGS Go Server.
 ( 13 convolutional layers policy network ).
 The second stage of policy network is trained using policy gradient reinforceme
nt learning.
 This reinforceement learning improved the policy network furthermore.
 
\end_layout

\begin_layout Standard
2.
 Value Network : Value network evaluate a particular position and determines
 who is winning ( 0 - white or 1 - black).
 The value network is also trained using reinforcement training.
 Initially using KGS data, the value network memorised the game outcomes
 rather than generalising to new positions.
 Achieving a minimum MSE(mean squared error) of 0.37 on the test set, compared
 to 0.19 on the training set.
 This is mainly because of the complexity of Go game - as successive positions
 are strongly correlated, differing by just one stone.
 To mitigate this problem, AlphaGo is made to play against each other and
 generated new self-play data set consisting of 30 million distinct positions,
 each sampled from a separate game.
 Later, the best version of Alpha is trained by playing against the previous
 best version of AlphGo, and eventually value network got better.
 
\end_layout

\begin_layout Standard
AlphaGo combines the policy and value networks in an MCTS (Monte Carlo tree
 search) algorithm that selects actions by lookahead search.
 Evaluating policy and value networks requires several orders of magnitude
 more computation than traditional search heuristics.
 To efficiently combine MCTS with deep neural networks, AlphaGo uses an
 asynchronous multi-threaded search that executes simulations on CPUs and
 computes policy and value networks in parallel on GPUs.
 
\end_layout

\begin_layout Standard
Summary of Results Notable matches of AlphaGO: AlphaGo was able to beat
 the strongest human player, thereby achieving one of artificial intelligence’s
 grand challenges.
 It’s important to note that AlphaGo evaluated thousands of times fewer
 positions than Deep Blue did in its chess match against Kasparov compensating
 by selecting those positions more intelligently, using the policy network,
 and evaluating them more precisely, using the value network—an approach
 that is perhaps closer to how humans play.
 Furthermore, while Deep Blue relied on a handcrafted evaluation function,
 the neural networks of AlphaGo are trained directly from gameplay purely
 through general-purpose supervised and reinforcement learning methods.
 
\end_layout

\end_body
\end_document
